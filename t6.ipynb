{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用Bilstm+Attention进行文本分类，依然使用Flyai的框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--EPOCHS\", default=10, type=int, help=\"train epochs\")\n",
    "parser.add_argument(\"-b\", \"--BATCH\", default=64, type=int, help=\"batch size\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "'''\n",
    "flyai库中的提供的数据处理方法\n",
    "传入整个数据训练多少轮，每批次批大小\n",
    "'''\n",
    "dataset = Dataset(epochs=args.EPOCHS, batch=args.BATCH)\n",
    "model = Model(dataset)\n",
    "\n",
    "# 超参\n",
    "vocab_size = 20655      # 总词汇量\n",
    "embedding_dim = 64      # 嵌入层大小\n",
    "hidden_dim = 128        # Dense层大小\n",
    "max_seq_len = 34        # 最大句长\n",
    "num_filters = 256       # 卷积核数目\n",
    "kernel_size = 5         # 卷积核尺寸\n",
    "learning_rate = 1e-3    # 学习率\n",
    "numclass = 3            # 类别数\n",
    "lstm_units = 64\n",
    "atten_size = 128\n",
    "\n",
    "# 传值空间\n",
    "input_x = tf.placeholder(tf.int32, shape=[None, max_seq_len], name='input_x')\n",
    "input_y = tf.placeholder(tf.int32, shape=[None], name='input_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "# define embedding layer\n",
    "with tf.variable_scope('embedding'):\n",
    "    # 标准正态分布初始化\n",
    "    input_embedding = tf.Variable(\n",
    "        tf.truncated_normal(shape=[vocab_size, embedding_dim], stddev=0.1), name='encoder_embedding')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义bilstm网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"bilstm\"):\n",
    "    # CNN layer\n",
    "    x_input_embedded = tf.nn.embedding_lookup(input_embedding, input_x)\n",
    "    # conv = tf.layers.conv1d(x_input_embedded, num_filters, kernel_size, name='conv')\n",
    "    # # global max pooling layer\n",
    "    # pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "    encode_fw = tf.nn.rnn_cell.BasicLSTMCell(lstm_units)\n",
    "    encode_fw = tf.nn.rnn_cell.DropoutWrapper(cell=encode_fw, output_keep_prob=keep_prob)\n",
    "    encode_bw = tf.nn.rnn_cell.BasicLSTMCell(lstm_units)\n",
    "    encode_bw = tf.nn.rnn_cell.DropoutWrapper(cell=encode_bw, output_keep_prob=keep_prob)\n",
    "    ((encode_fw_output, encode_bw_output),\n",
    "     (encode_fw_state, encode_bw_state)) = (tf.nn.bidirectional_dynamic_rnn(cell_fw=encode_fw,\n",
    "                                                                            cell_bw=encode_bw,\n",
    "                                                                            inputs = x_input_embedded,\n",
    "                                                                            dtype=tf.float32))\n",
    "\n",
    "    output = tf.concat((encode_fw_output, encode_bw_output), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义attention网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('attention'):\n",
    "    att_in = tf.expand_dims(output, axis=2)\n",
    "    w_att = tf.Variable(tf.random_normal([1, 1, 2 * lstm_units, atten_size], stddev=0.1))\n",
    "    b_att = tf.Variable(tf.random_normal([atten_size], stddev=0.1))\n",
    "    u_att = tf.Variable(tf.random_normal([1, 1, atten_size, 1], stddev=0.1))\n",
    "    v_att = tf.tanh(tf.nn.conv2d(att_in, w_att, strides=[1, 1, 1, 1], padding='SAME') + b_att)\n",
    "    betas = tf.nn.conv2d(v_att, u_att, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    exp_betas = tf.reshape(tf.exp(betas), [-1, max_seq_len])\n",
    "    alphas = exp_betas / tf.reshape(tf.reduce_mean(exp_betas, 1), [-1, 1])\n",
    "    last = tf.reduce_sum(output * tf.reshape(alphas, [-1, max_seq_len, 1]), 1)\n",
    "with tf.name_scope(\"score\"):\n",
    "    # 全连接层，后面接dropout以及relu激活\n",
    "    weight = tf.Variable(tf.random_normal([2 * lstm_units, numclass], stddev=0.1))\n",
    "    bias = tf.Variable(tf.random_normal([numclass], stddev=0.1))\n",
    "    y_ = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "\n",
    "    # 分类器\n",
    "\n",
    "    y_pred_cls = tf.argmax(tf.nn.softmax(y_), 1, name='y_pred')  # 预测类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"score\"):\n",
    "    # 全连接层，后面接dropout以及relu激活\n",
    "    weight = tf.Variable(tf.random_normal([2 * lstm_units, numclass], stddev=0.1))\n",
    "    bias = tf.Variable(tf.random_normal([numclass], stddev=0.1))\n",
    "    y_ = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "\n",
    "    # 分类器\n",
    "\n",
    "    y_pred_cls = tf.argmax(tf.nn.softmax(y_), 1, name='y_pred')  # 预测类别\n",
    "\n",
    "with tf.name_scope(\"optimize\"):\n",
    "    # 将label进行onehot转化\n",
    "    one_hot_labels = tf.one_hot(input_y, depth=numclass, dtype=tf.float32)\n",
    "    # 损失函数，交叉熵\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=one_hot_labels)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # 优化器\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # 准确率\n",
    "    correct_pred = tf.equal(tf.argmax(one_hot_labels, 1), y_pred_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='acc')\n",
    "\n",
    "with tf.name_scope(\"summary\"):\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "best_score = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "    # dataset.get_step() 获取数据的总迭代次数\n",
    "    for step in range(dataset.get_step()):\n",
    "        x_train, y_train = dataset.next_train_batch()\n",
    "        # x_val, y_val = dataset.next_validation_batch()\n",
    "\n",
    "        fetches = [loss, accuracy, train_op]\n",
    "        feed_dict = {input_x: x_train, input_y: y_train, keep_prob: 0.9}\n",
    "        loss_, accuracy_, _ = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "        summary = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "        cur_step = str(step + 1) + \"/\" + str(dataset.get_step())\n",
    "        print('The Current step per total: {} | The Current loss: {} | The Current ACC: {}'.\n",
    "              format(cur_step, loss_, accuracy_))\n",
    "        if step % 100 == 0:\n",
    "            model.save_model(sess, MODEL_PATH, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Current step per total: 11/20 | The Current loss: 0.5396712422370911 | The Current ACC: 0.796875\n",
    "The Current step per total: 12/20 | The Current loss: 0.5476027727127075 | The Current ACC: 0.796875\n",
    "The Current step per total: 13/20 | The Current loss: 0.5575822591781616 | The Current ACC: 0.796875\n",
    "The Current step per total: 14/20 | The Current loss: 0.5402854681015015 | The Current ACC: 0.796875\n",
    "The Current step per total: 15/20 | The Current loss: 0.5028542280197144 | The Current ACC: 0.8125\n",
    "The Current step per total: 16/20 | The Current loss: 0.4492661952972412 | The Current ACC: 0.828125\n",
    "The Current step per total: 17/20 | The Current loss: 0.4068627953529358 | The Current ACC: 0.828125\n",
    "The Current step per total: 18/20 | The Current loss: 0.3983329236507416 | The Current ACC: 0.859375\n",
    "The Current step per total: 19/20 | The Current loss: 0.4089481234550476 | The Current ACC: 0.890625\n",
    "The Current step per total: 20/20 | The Current loss: 0.38450390100479126 | The Current ACC: 0.921875\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出结果不错"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
