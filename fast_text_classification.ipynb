{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我使用了flyai这个竞赛平台进行文本分类，我使用的是用tensorflow实现的fasttext网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法的大致框架如下，如下部分是构建fasttext网络，结构很简单，只需要一层简单的网络层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.variable_scope('embedding'):\n",
    "    # 标准正态分布初始化\n",
    "    input_embedding = tf.Variable(\n",
    "        tf.truncated_normal(shape=[vocab_size, embedding_dim], stddev=0.1), name='encoder_embedding')\n",
    "\n",
    "with tf.name_scope(\"fasttext\"):\n",
    "    \n",
    "    x_input_embedded = tf.nn.embedding_lookup(input_embedding, input_x)\n",
    "    # conv = tf.layers.conv1d(x_input_embedded, num_filters, kernel_size, name='conv')\n",
    "    # # global max pooling layer\n",
    "    # pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "    sentence_embedd = tf.reduce_mean(x_input_embedded, axis=1)\n",
    "    w = tf.get_variable('W', [embedding_dim, numclass])\n",
    "    b = tf.get_variable('b', [numclass])\n",
    "    logits = tf.matmul(sentence_embedd, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是代码的损失函数精度，以及训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2_reg_lambda = 0.0\n",
    "with tf.name_scope(\"score\"):\n",
    "    # 全连接层，后面接dropout以及relu激活\n",
    "    # fc = tf.layers.dense(pooling, hidden_dim, name='fc1')\n",
    "    # fc = tf.contrib.layers.dropout(fc, keep_prob)\n",
    "    # fc = tf.nn.relu(fc)\n",
    "    # loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n",
    "    # L2_loss = tf.nn.l2_loss(w)\n",
    "    # L2_loss += tf.nn.l2_loss(b)\n",
    "    # loss += L2_loss * l2_reg_lambda\n",
    "\n",
    "\n",
    "    # # 分类器\n",
    "    # logits = tf.layers.dense(fc, numclass, name='fc2')\n",
    "\n",
    "    y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1, name='y_pred')  # 预测类别\n",
    "\n",
    "with tf.name_scope(\"optimize\"):\n",
    "    # 将label进行onehot转化\n",
    "    one_hot_labels = tf.one_hot(input_y, depth=numclass, dtype=tf.float32)\n",
    "    # 损失函数，交叉熵\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_labels)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # 优化器\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # 准确率\n",
    "    correct_pred = tf.equal(tf.argmax(one_hot_labels, 1), y_pred_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='acc')\n",
    "\n",
    "with tf.name_scope(\"summary\"):\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "best_score = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "    # dataset.get_step() 获取数据的总迭代次数\n",
    "    for step in range(dataset.get_step()):\n",
    "        x_train, y_train = dataset.next_train_batch()\n",
    "        # x_val, y_val = dataset.next_validation_batch()\n",
    "\n",
    "        fetches = [loss, accuracy, train_op]\n",
    "        feed_dict = {input_x: x_train, input_y: y_train, keep_prob: 0.9}\n",
    "        loss_, accuracy_, _ = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "        summary = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "        cur_step = str(step + 1) + \"/\" + str(dataset.get_step())\n",
    "        print('The Current step per total: {} | The Current loss: {} | The Current ACC: {}'.\n",
    "              format(cur_step, loss_, accuracy_))\n",
    "        if step % 100 == 0:\n",
    "            model.save_model(sess, MODEL_PATH, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看看训练过程的日志，分类比其他的网络都要快，效果也不错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Current step per total: 459/500 | The Current loss: 0.1529412865638733 | The Current ACC: 1.0\n",
    "The Current step per total: 460/500 | The Current loss: 0.5357775092124939 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 461/500 | The Current loss: 0.3405835032463074 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 462/500 | The Current loss: 0.15472419559955597 | The Current ACC: 1.0\n",
    "The Current step per total: 463/500 | The Current loss: 0.15399295091629028 | The Current ACC: 1.0\n",
    "The Current step per total: 464/500 | The Current loss: 0.303913414478302 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 465/500 | The Current loss: 0.19542500376701355 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 466/500 | The Current loss: 0.25718802213668823 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 467/500 | The Current loss: 0.41457900404930115 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 468/500 | The Current loss: 0.590376079082489 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 469/500 | The Current loss: 0.1942882090806961 | The Current ACC: 1.0\n",
    "The Current step per total: 470/500 | The Current loss: 0.14795874059200287 | The Current ACC: 1.0\n",
    "The Current step per total: 471/500 | The Current loss: 0.5234373807907104 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 472/500 | The Current loss: 0.33145222067832947 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 473/500 | The Current loss: 0.1468111276626587 | The Current ACC: 1.0\n",
    "The Current step per total: 474/500 | The Current loss: 0.14611704647541046 | The Current ACC: 1.0\n",
    "The Current step per total: 475/500 | The Current loss: 0.29498711228370667 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 476/500 | The Current loss: 0.18875113129615784 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 477/500 | The Current loss: 0.24813206493854523 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 478/500 | The Current loss: 0.4078347682952881 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 479/500 | The Current loss: 0.5791069865226746 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 480/500 | The Current loss: 0.18591205775737762 | The Current ACC: 1.0\n",
    "The Current step per total: 481/500 | The Current loss: 0.14316657185554504 | The Current ACC: 1.0\n",
    "The Current step per total: 482/500 | The Current loss: 0.511402428150177 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 483/500 | The Current loss: 0.3227069079875946 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 484/500 | The Current loss: 0.13929709792137146 | The Current ACC: 1.0\n",
    "The Current step per total: 485/500 | The Current loss: 0.13863947987556458 | The Current ACC: 1.0\n",
    "The Current step per total: 486/500 | The Current loss: 0.28622403740882874 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 487/500 | The Current loss: 0.18225839734077454 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 488/500 | The Current loss: 0.23940058052539825 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 489/500 | The Current loss: 0.40118408203125 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 490/500 | The Current loss: 0.5678361654281616 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 491/500 | The Current loss: 0.17787066102027893 | The Current ACC: 1.0\n",
    "The Current step per total: 492/500 | The Current loss: 0.13856475055217743 | The Current ACC: 1.0\n",
    "The Current step per total: 493/500 | The Current loss: 0.4996697008609772 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 494/500 | The Current loss: 0.31433865427970886 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 495/500 | The Current loss: 0.13217487931251526 | The Current ACC: 1.0\n",
    "The Current step per total: 496/500 | The Current loss: 0.1315530240535736 | The Current ACC: 1.0\n",
    "The Current step per total: 497/500 | The Current loss: 0.27763742208480835 | The Current ACC: 0.800000011920929\n",
    "The Current step per total: 498/500 | The Current loss: 0.1759549379348755 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 499/500 | The Current loss: 0.23099246621131897 | The Current ACC: 0.8999999761581421\n",
    "The Current step per total: 500/500 | The Current loss: 0.394628643989563 | The Current ACC: 0.8999999761581421\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
